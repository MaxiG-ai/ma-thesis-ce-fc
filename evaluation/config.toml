# Nestful Benchmark Evaluation Configuration

model_names = ["llama3.2:3b"]

[nestful]
provider = "ollama"

# Dataset configuration
dataset = "datasets/nestful/data_v2/nestful_data.jsonl"

# Generation parameters
temperature = 0.3
max_tokens = 1000
batch_size = 1

# In-context learning
icl_count = 3

# Data limit (number of examples to process, set to null or omit for all data)
data_limit = 15

# Output configuration
save_directory = "results"

[nestful.supported_models]
GRANITE_MODELS = [
    "granite-3.0-8b-instruct",
    "granite-3.0-8b-instruct-FT",
]

GRANITE_3_1_MODELS = [
    "granite-3.1-8b-instruct"
]

DEEPSEEK = [
    "DeepSeek-V3",
    "DeepSeek-R1"
]

LLAMA_MODELS = [
    "llama3.2:3b",
    "llama3.1:8b",
    "Llama-3.1-8B-Instruct",
    "llama-3-1-70b-instruct",
    "llama-3-1-405b-instruct-fp8",
    "Llama-3.2-11B-Vision-Instruct",
    "Llama-3.2-90B-Vision-Instruct",
]


# MCP Bench Benchmark Evaluation Configuration

[mcpbench]
provider = "ollama"

# Dataset configuration
tasks_file = "datasets/mcp_bench/tasks/mcpbench_tasks_single_runner_format.json"

# Generation parameters
temperature = 0.3
max_tokens = 1000

# Task limit (number of tasks to process, set to null or omit for all tasks)
task_limit = 5

# Server configuration
enable_distraction_servers = false
distraction_count = 0

# Evaluation configuration
enable_judge_stability = true
filter_problematic_tools = true
concurrent_summarization = true
use_fuzzy_descriptions = false

# Output configuration
results_dir = "results/mcpbench"