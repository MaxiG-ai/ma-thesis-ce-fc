# Multi-Component Evaluation Configuration
# This configuration supports comprehensive evaluation across:
# - Multiple models
# - Multiple memory methods  
# - Multiple benchmarks

# Global Settings
model_names = ["llama3.2:3b"] # , "llama3.1:8b"
memory_methods = ["truncation"]
benchmarks = ["nestful"] #, "mcpbench"

# Evaluation Configuration
results_dir = "results"
concurrent_evaluations = 1  # Number of parallel evaluations

# Provider Configurations
[ollama_config]
temperature = 0.3
max_tokens = 1000

[openrouter_config] 
temperature = 0.3
max_tokens = 1000

# Memory Method Configurations
[truncation_config]
max_tokens = 500

# Benchmark Configurations
[nestful]
provider = "ollama"

# Dataset configuration
dataset = "datasets/nestful/data_v2/nestful_data.jsonl"

# Generation parameters
temperature = 0.3
max_tokens = 1000
batch_size = 1

# In-context learning
icl_count = 3

# Data limit (number of examples to process, set to null or omit for all data)
data_limit = 15

# Output configuration
save_directory = "results"

[nestful.supported_models]
GRANITE_MODELS = [
    "granite-3.0-8b-instruct",
    "granite-3.0-8b-instruct-FT",
]

GRANITE_3_1_MODELS = [
    "granite-3.1-8b-instruct"
]

DEEPSEEK = [
    "DeepSeek-V3",
    "DeepSeek-R1"
]

LLAMA_MODELS = [
    "llama3.2:3b",
    "llama3.1:8b",
    "Llama-3.1-8B-Instruct",
    "llama-3-1-70b-instruct", 
    "llama-3-1-405b-instruct-fp8",
    "Llama-3.2-11B-Vision-Instruct",
    "Llama-3.2-90B-Vision-Instruct",
]

# MCP Bench Benchmark Configuration
[mcpbench]
provider = "ollama"

# Dataset configuration
tasks_file = "datasets/mcp_bench/tasks/mcpbench_tasks_single_runner_format.json"

# Generation parameters
temperature = 0.3
max_tokens = 1000

# Task limit (number of tasks to process, set to null or omit for all tasks)
task_limit = 5

# Server configuration
enable_distraction_servers = false
distraction_count = 0

# Evaluation configuration
enable_judge_stability = true
filter_problematic_tools = true
concurrent_summarization = true
use_fuzzy_descriptions = false

# Output configuration
results_dir = "results/mcpbench"

# Legacy Support (for backward compatibility)
selected_benchmarks = ["nestful"]  # Deprecated: use 'benchmarks' instead