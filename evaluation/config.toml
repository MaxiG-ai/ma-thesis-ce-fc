# Multi-Component Evaluation Configuration
# This configuration supports comprehensive evaluation across:
# - Multiple models
# - Multiple memory methods  
# - Multiple benchmarks

# Global Settings
memory_methods = ["truncation"]
executed_benchmarks = ["nestful"] #, "mcpbench"]

# Evaluation Configuration
results_dir = "results"
concurrent_evaluations = 1  # Number of parallel evaluations

# Provider-specific configurations
[providers.ollama]
models = ["gemma3:270m", "llama3.2:3b", ]
enabled_models = [] # ["llama3.2:3b"]
temperature = 0.3
max_tokens = 5000

[providers.openrouter]
models = ["google/gemma-3-27b-it:free"]
enabled_models = ["google/gemma-3-27b-it:free"]
temperature = 0.3
max_tokens = 8000


# Memory Method Configurations
[memoryMethods.truncation]
max_tokens = 500

# Benchmark Configurations
[benchmarks.nestful]
dataset = "data_v2/nestful_data.jsonl"
icl_examples_path = "src/icl_examples.json"
prompts_path = "src/PROMPTS.json"
temperature = 0.0
max_tokens = 1000
batch_size = 1
icl_count = 3
data_limit = 15
save_directory = "results"
demo = false

# Using inline table syntax for supported_models
supported_models = { GRANITE_MODELS = ["granite-3.0-8b-instruct", "granite-3.0-8b-instruct-FT"], GRANITE_3_1_MODELS = ["granite-3.1-8b-instruct"], DEEPSEEK = ["DeepSeek-V3", "DeepSeek-R1"], LLAMA_MODELS = ["llama3.2:3b", "llama3.1:8b", "Llama-3.1-8B-Instruct", "llama-3-1-70b-instruct", "llama-3-1-405b-instruct-fp8", "Llama-3.2-11B-Vision-Instruct", "Llama-3.2-90B-Vision-Instruct"] }

# MCP Bench Benchmark Configuration
[benchmarks.mcpbench]
tasks_file = "datasets/mcp_bench/tasks/mcpbench_tasks_single_runner_format.json"
temperature = 0.3
max_tokens = 1000
task_limit = 5
enable_distraction_servers = false
distraction_count = 0
enable_judge_stability = true
filter_problematic_tools = true
concurrent_summarization = true
use_fuzzy_descriptions = false
results_dir = "results/mcpbench"
demo = false
